"""
Example Inference Framework
-----------------------------------------
This script demonstrates a possible structure for:
1) Loading segmentation models,
2) Running segmentation on knee radiographs,
3) Extracting patches around predicted joint centers,
4) Running multiple classification models on each patch,
5) Aggregating results into a CSV file.

This code focuses on showing the pipeline organization.
It does not describe training details or implementation choices outside the scope
of inference workflow demonstration.
"""

import os as _os
import re as _re
import cv2 as _cv
import numpy as _np
import pandas as _pd
import torch as _t
import torch.nn.functional as _F
import torchvision.models as _vm
from torch import nn as _nn
from torchvision import transforms as _tf


# -------------------------------------------------------------
# Global configuration parameters
# -------------------------------------------------------------
_DEVICE = "cuda" if _t.cuda.is_available() else "cpu"
_TO_TENSOR = _tf.ToTensor()

# Image and patch configuration
_IMG_SIZE = 512
_PATCH_RADIUS = 96
_SEG_THRESH = 0.3

# Base paths for models and data
_BASE_DIR = r"/project/data"
_MODEL_A_PATH = _os.path.join(_BASE_DIR, "moduleA", "modelA_v1.pth")
_MODEL_B_PATH = _os.path.join(_BASE_DIR, "moduleB", "modelB_v1.pth")
_CLASS_DIR = _os.path.join(_BASE_DIR, "class_models")
_IMG_ROOT = _os.path.join(_BASE_DIR, "images")
_OUT_CSV = _os.path.join(_BASE_DIR, "results", "summary.csv")

# Tag list for multiple classification heads
_SUBTAGS = ["T01","T02","T03","T04","T05","T06","T07","T08","T09","T10","T11"]


# -------------------------------------------------------------
# UNet-like segmentation network
# -------------------------------------------------------------
class A1(_nn.Module):
    """
    UNet-style segmentation model.
    The model takes a single-channel image and produces a single-channel
    probability map of the same spatial resolution.
    """

    def __init__(self):
        super(A1, self).__init__()

        # Encoder stages
        self.l1 = self._block(1, 64)
        self.l2 = self._block(64, 128)
        self.l3 = self._block(128, 256)
        self.l4 = self._block(256, 512)
        self.l5 = self._block(512, 1024)

        # Decoder stages
        self.u4 = _nn.ConvTranspose2d(1024, 512, 2, 2)
        self.d4 = self._block(1024, 512)
        self.u3 = _nn.ConvTranspose2d(512, 256, 2, 2)
        self.d3 = self._block(512, 256)
        self.u2 = _nn.ConvTranspose2d(256, 128, 2, 2)
        self.d2 = self._block(256, 128)
        self.u1 = _nn.ConvTranspose2d(128, 64, 2, 2)
        self.d1 = self._block(128, 64)

        # Final 1×1 output layer
        self.out = _nn.Conv2d(64, 1, 1)

    def _block(self, in_ch, out_ch):
        """
        A standard two-layer convolutional block with:
        Conv → BN → ReLU → Conv → BN → ReLU.
        """
        return _nn.Sequential(
            _nn.Conv2d(in_ch, out_ch, 3, padding=1),
            _nn.BatchNorm2d(out_ch),
            _nn.ReLU(True),
            _nn.Conv2d(out_ch, out_ch, 3, padding=1),
            _nn.BatchNorm2d(out_ch),
            _nn.ReLU(True),
        )

    def forward(self, x):
        # Encoder path
        a1 = self.l1(x)
        a2 = self.l2(_F.max_pool2d(a1, 2))
        a3 = self.l3(_F.max_pool2d(a2, 2))
        a4 = self.l4(_F.max_pool2d(a3, 2))
        b  = self.l5(_F.max_pool2d(a4, 2))

        # Decoder path with skip connections
        y4 = self.u4(b)
        y4 = self.d4(_t.cat((y4, a4), 1))

        y3 = self.u3(y4)
        y3 = self.d3(_t.cat((y3, a3), 1))

        y2 = self.u2(y3)
        y2 = self.d2(_t.cat((y2, a2), 1))

        y1 = self.u1(y2)
        y1 = self.d1(_t.cat((y1, a1), 1))

        # Output probability map
        return _t.sigmoid(self.out(y1))


# -------------------------------------------------------------
# Load segmentation models A and B
# -------------------------------------------------------------
print(">> Loading UNet A <<")
_modA = A1().to(_DEVICE)
_modA.load_state_dict(_t.load(_MODEL_A_PATH, weights_only=True))
_modA.eval()

print(">> Loading UNet B <<")
_modB = A1().to(_DEVICE)
_modB.load_state_dict(_t.load(_MODEL_B_PATH, weights_only=True))
_modB.eval()


# -------------------------------------------------------------
# Load a ResNet-based classifier for each tag
# -------------------------------------------------------------
def load_resnet(path):
    """
    Loads a ResNet-50 classifier for one specific tag.

    Steps:
    1) Load a ResNet-50 backbone.
    2) Replace the final FC layer according to checkpoint shape.
    3) Replace input conv to support 1-channel input.
    4) Load checkpoint weights.
    """
    m = _vm.resnet50(weights=_vm.ResNet50_Weights.DEFAULT)

    ck = _t.load(path, weights_only=True)
    num_classes = ck["fc.weight"].shape[0]

    m.fc = _nn.Linear(m.fc.in_features, num_classes)
    m.conv1 = _nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)

    m.load_state_dict(ck)
    m = m.to(_DEVICE)
    m.eval()

    return m


# Load all classification models
_CLASSIFIERS = {}
for tag in _SUBTAGS:
    fp = _os.path.join(_CLASS_DIR, tag, f"{tag}_best.pth")
    if _os.path.exists(fp):
        _CLASSIFIERS[tag] = load_resnet(fp)
    else:
        print(f"Warning: missing classifier {fp}")


# -------------------------------------------------------------
# Compute contour centroids from a binary mask
# -------------------------------------------------------------
def extract_centroids(mask):
    """
    Given a binary mask (uint8, 0 or 255), extract contour centroids
    using image moments.
    """
    contours, _ = _cv.findContours(mask, _cv.RETR_EXTERNAL, _cv.CHAIN_APPROX_SIMPLE)
    centers = []

    for c in contours:
        m = _cv.moments(c)
        if m["m00"] != 0:
            cx = int(m["m10"] / m["m00"])
            cy = int(m["m01"] / m["m00"])
            centers.append((cx, cy))

    return centers


# -------------------------------------------------------------
# Extract local patches around predicted centers
# -------------------------------------------------------------
def extract_patches(img, centers):
    """
    For each center (x, y):
    1) Extract a square region of width 2 * PATCH_RADIUS,
    2) Clip boundaries,
    3) Resize to 512×512,
    4) Return a list of (patch_image, x_position).
    """
    patches = []

    for x, y in centers:
        x0 = max(x - _PATCH_RADIUS, 0)
        x1 = min(x + _PATCH_RADIUS, img.shape[1])
        y0 = max(y - _PATCH_RADIUS, 0)
        y1 = min(y + _PATCH_RADIUS, img.shape[0])

        patch = img[y0:y1, x0:x1]
        patch = _cv.resize(patch, (_IMG_SIZE, _IMG_SIZE))

        patches.append((patch, x))

    return patches


# -------------------------------------------------------------
# Process a single image
# -------------------------------------------------------------
def process_image(filepath):
    """
    Performs inference on one radiograph:

    Steps:
    1) Parse patient ID from filename.
    2) Load image in grayscale.
    3) Resize and center-crop to 512×512.
    4) Apply UNet A and B for segmentation.
    5) Extract centroids and choose the set with more points.
    6) Extract patches around centroids.
    7) Run each patch through all classification models.
    8) Return aggregated left/right predictions for each tag.
    """
    filename = _os.path.basename(filepath)
    clean_name = _re.sub(r"[\^\s]", "", filename)

    # Expect a specific naming pattern
    if "OAIXRAYSCREENINGKNEE" not in clean_name:
        return None, None

    pid = clean_name.split("_")[0]

    # Load grayscale image
    img = _cv.imread(filepath, _cv.IMREAD_GRAYSCALE)
    if img is None:
        return None, None

    # Resize to maintain min dimension = 512, then center-crop
    h, w = img.shape
    if min(h, w) != _IMG_SIZE:
        scale = _IMG_SIZE / min(h, w)
        nh, nw = int(h * scale), int(w * scale)
        img = _cv.resize(img, (nw, nh))

        cx, cy = nw // 2, nh // 2
        img = img[cy - 256:cy + 256, cx - 256:cx + 256]

    # Convert to tensor
    t = _TO_TENSOR(img).unsqueeze(0).to(_DEVICE)

    # Segmentation masks from model A
    try:
        probA = _modA(t).squeeze().cpu().numpy()
        maskA = (probA > _SEG_THRESH).astype(_np.uint8) * 255
    except RuntimeError:
        return None, None

    # Segmentation masks from model B
    probB = _modB(t).squeeze().cpu().numpy()
    maskB = (probB > _SEG_THRESH).astype(_np.uint8) * 255

    # Extract centroids from both masks
    centersA = extract_centroids(maskA)
    centersB = extract_centroids(maskB)

    # Select the set with more centers
    centers = centersA if len(centersA) >= len(centersB) else centersB

    # Extract local image patches
    patches = extract_patches(img, centers)

    # Classification result structure
    result = {}

    for tag, model in _CLASSIFIERS.items():
        pred_lr = {"L": None, "R": None}

        for patch, x in patches:
            side = "L" if x < (_IMG_SIZE // 2) else "R"

            tensor_patch = _TO_TENSOR(patch).unsqueeze(0).to(_DEVICE)
            logits = model(tensor_patch)
            pred = int(_t.argmax(logits, 1).cpu().item())

            pred_lr[side] = pred

        result[tag] = pred_lr

    return pid, result


# -------------------------------------------------------------
# Batch processing loop
# -------------------------------------------------------------
rows = []

for root, _, files in _os.walk(_IMG_ROOT):
    for fname in files:
        pid, res = process_image(_os.path.join(root, fname))
        if pid is None:
            continue

        row = {"Patient": pid}

        # Flatten classification results: e.g. T01_L, T01_R
        for tag, sides in res.items():
            for side, val in sides.items():
                row[f"{tag}_{side}"] = val

        rows.append(row)

# Convert collected rows into DataFrame
_df = _pd.DataFrame(rows)
_df.to_csv(_OUT_CSV, index=False)

print(f">> Results saved to {_OUT_CSV} <<")
