import os
import copy
import time
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from torchvision import transforms, datasets, models
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix, roc_curve, auc
from sklearn.preprocessing import label_binarize

# -------------------------
# Global settings (font, figure size, etc.)
# -------------------------
plt.rcParams['font.family'] = 'Arial'
plt.rcParams.update({'font.size': 14})

# -------------------------
# Windowing transform (e.g., for medical images)
# -------------------------
class WindowingTransform:
    def __init__(self, center=100, width=400):
        # Store parameters
        self.min_val = center - width / 2
        self.max_val = center + width / 2

    def __call__(self, img):
        # Convert to array, clip, normalize and convert back to tensor
        arr = np.array(img).astype(np.float32)
        arr = np.clip(arr, self.min_val, self.max_val)
        arr = (arr - self.min_val) / (self.max_val - self.min_val)
        if arr.ndim == 2:
            arr = np.stack([arr] * 3, axis=-1)
        return torch.from_numpy(arr.transpose(2, 0, 1))

# -------------------------
# Early stopping utility
# -------------------------
class EarlyStopper:
    def __init__(self, patience=10, min_delta=1e-6):
        # Initialize counters and thresholds
        self.patience = patience
        self.min_delta = min_delta
        self.best_loss = float('inf')
        self.counter = 0
        self.best_weights = None
        self.should_stop = False

    def __call__(self, val_loss, model):
        # Compare validation loss to best, update or increment counter
        if val_loss + self.min_delta < self.best_loss:
            self.best_loss = val_loss
            self.best_weights = copy.deepcopy(model.state_dict())
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.should_stop = True

# -------------------------
# Training loop
# -------------------------
def train_model(
    model,
    dataloaders,
    criterion,
    optimizer,
    scheduler,
    num_epochs=25,
    patience=5,
    device='cpu'
):
    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
    stopper = EarlyStopper(patience=patience)
    best_acc = 0.0

    for epoch in range(num_epochs):
        for phase in ['train', 'val']:
            model.train() if phase == 'train' else model.eval()
            running_loss = 0.0
            running_correct = 0
            total_samples = 0

            for inputs, labels in dataloaders[phase]:
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer.zero_grad()
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    preds = outputs.argmax(dim=1)
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()
                running_loss += loss.item() * inputs.size(0)
                running_correct += (preds == labels).sum().item()
                total_samples += inputs.size(0)

            epoch_loss = running_loss / total_samples
            epoch_acc = running_correct / total_samples
            history[f'{phase}_loss'].append(epoch_loss)
            history[f'{phase}_acc'].append(epoch_acc)

            if phase == 'val':
                stopper(epoch_loss, model)
                if epoch_acc > best_acc:
                    best_acc = epoch_acc
                if stopper.should_stop:
                    model.load_state_dict(stopper.best_weights)
                    return model, history
            else:
                scheduler.step()

    return model, history

# -------------------------
# Evaluation function
# -------------------------
def evaluate_model(model, dataloader, device='cpu'):
    model.eval()
    true_labels = []
    predictions = []
    probabilities = []
    with torch.no_grad():
        for inputs, labels in dataloader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            probs = torch.softmax(outputs, dim=1)
            preds = outputs.argmax(dim=1)
            true_labels.extend(labels.cpu().tolist())
            predictions.extend(preds.cpu().tolist())
            probabilities.extend(probs.cpu().numpy())
    return np.array(true_labels), np.array(predictions), np.array(probabilities)

# -------------------------
# Main execution
# -------------------------
def main():
    # Use environment variables or defaults for paths
    root_dir = os.getenv('DATA_ROOT', '/path/to/data')
    results_dir = os.getenv('RESULTS_ROOT', '/path/to/results')
    os.makedirs(results_dir, exist_ok=True)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Define transforms
    transforms_cfg = {
        'train': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.RandomCrop(224),
            transforms.RandomHorizontalFlip(),
            WindowingTransform(),
            transforms.Normalize([0.5] * 3, [0.5] * 3),
        ]),
        'val': transforms.Compose([
            transforms.Resize((256, 256)),
            transforms.CenterCrop(224),
            WindowingTransform(),
            transforms.Normalize([0.5] * 3, [0.5] * 3),
        ]),
    }

    # Iterate over tasks/datasets
    for task_name in os.listdir(root_dir):
        task_path = os.path.join(root_dir, task_name)
        if not os.path.isdir(task_path):
            continue

        # Prepare output folder per task
        task_out = os.path.join(results_dir, task_name)
        os.makedirs(task_out, exist_ok=True)

        # Load full dataset and get number of classes
        full_ds = datasets.ImageFolder(task_path, transform=transforms_cfg['train'])
        num_classes = len(full_ds.classes)

        # 5-fold cross-validation
        kfold = KFold(n_splits=5, shuffle=True, random_state=42)
        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(full_ds)):
            # Create subsets and dataloaders
            train_ds = Subset(full_ds, train_idx)
            val_ds = Subset(full_ds, val_idx)
            train_ds.dataset.transform = transforms_cfg['train']
            val_ds.dataset.transform = transforms_cfg['val']
            loaders = {
                'train': DataLoader(train_ds, batch_size=32, shuffle=True),
                'val': DataLoader(val_ds, batch_size=32, shuffle=False),
            }

            # Initialize model and replace final layer
            model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
            model.fc = nn.Linear(model.fc.in_features, num_classes)
            model.to(device)

            # Define loss, optimizer, and scheduler
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(model.parameters(), lr=1e-5)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

            # Train and evaluate
            trained_model, history = train_model(
                model, loaders, criterion, optimizer, scheduler,
                num_epochs=50, patience=10, device=device
            )

            # Save model and metrics (paths are placeholders)
            model_path = os.path.join(task_out, f"model_fold{fold_idx+1}.pth")
            torch.save(trained_model.state_dict(), model_path)

            # (Plotting and analysis steps would follow, using generic save paths)

if __name__ == '__main__':
    main()
