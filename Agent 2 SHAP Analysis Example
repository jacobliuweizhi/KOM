import os
import pickle
import joblib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import shap

from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier  # noqa: F401  # Optional: if you use XGBoost classifiers


# ======================================================
# 0. Global plotting configuration
# ======================================================

plt.rcParams['font.family'] = 'Arial'
plt.rcParams['axes.linewidth'] = 1.5
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['xtick.labelsize'] = 10
plt.rcParams['ytick.labelsize'] = 10

# Optional: Nature-like color palette for custom plots
NATURE_COLORS = [
    '#E64B35', '#4DBBD5', '#00A087', '#3C5488',
    '#F39B7F', '#8491B4', '#91D1C2', '#DC0000'
]

# Optional: Adjust SHAP default colors
shap.plots.colors.blue_rgb = np.array([0, 160, 135]) / 255.0
shap.plots.colors.red_rgb = np.array([230, 75, 53]) / 255.0


# ======================================================
# Utility: unified save function for figures
# ======================================================

def save_plot(fig, filename, dpi=600):
    """
    Save the current matplotlib figure with consistent settings.

    Parameters
    ----------
    fig : matplotlib.figure.Figure
        Figure object to save.
    filename : str
        Full path to the output file.
    dpi : int
        Resolution of the saved figure.
    """
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    plt.tight_layout()
    fig.savefig(filename, dpi=dpi, bbox_inches='tight', facecolor='white')
    plt.close(fig)
    print(f"Saved: {filename}")


# ======================================================
# 1. LightGBM (or other tree-based) regression model SHAP analysis
#    Single-sample explanation
# ======================================================

def run_shap_for_lightgbm_single_patient():
    """
    Run SHAP analysis for a single subject using a trained regression model
    (e.g., LightGBM regressor).

    Assumptions:
    - The model was trained on standardized continuous features using the same StandardScaler.
    - The feature names and encodings match those used in training.
    """

    # ------------------ 1.1 Paths and I/O configuration ------------------ #
    # TODO: Replace these placeholders with your actual paths.
    DATA_PATH = r"PATH_TO_YOUR_TRAINING_DATA.csv"
    MODEL_PATH = r"PATH_TO_YOUR_REGRESSION_MODEL.pkl"
    RESULT_DIR = r"PATH_TO_OUTPUT_DIRECTORY_FOR_REGRESSION_SHAP"
    os.makedirs(RESULT_DIR, exist_ok=True)

    OUTPUT_FILES = {
        'force_html': os.path.join(RESULT_DIR, "shap_force_plot.html"),
        'force_png': os.path.join(RESULT_DIR, "shap_force_plot.png"),
        'waterfall': os.path.join(RESULT_DIR, "shap_waterfall_plot.png"),
        'bar': os.path.join(RESULT_DIR, "shap_bar_plot.png"),
    }

    # ------------------ 1.2 Feature definitions ------------------ #
    # TODO: Replace these with your actual feature lists.
    CATEGORICAL_FEATURES = [
        # "cat_feature_1", "cat_feature_2", ...
    ]
    CONTINUOUS_FEATURES = [
        # "cont_feature_1", "cont_feature_2", ...
    ]
    FEATURES = CATEGORICAL_FEATURES + CONTINUOUS_FEATURES

    # ------------------ 1.3 Load data & fit scaler ------------------ #
    # This should mirror the preprocessing used during model training.

    df_raw = pd.read_csv(DATA_PATH)

    # Drop rows with missing values in continuous features (if that matches training pipeline)
    if CONTINUOUS_FEATURES:
        df_raw = df_raw.dropna(subset=CONTINUOUS_FEATURES)

    scaler = StandardScaler()
    if CONTINUOUS_FEATURES:
        scaler.fit(df_raw[CONTINUOUS_FEATURES])

    # ------------------ 1.4 Build single-sample input ------------------ #
    # TODO: Construct a dict for a single subject with all required features.
    # NOTE: All feature names and encodings must be consistent with training.

    sample_data = {
        # "cat_feature_1": 0,
        # "cat_feature_2": 1,
        # "cont_feature_1": 50.0,
        # "cont_feature_2": 1.23,
        # ...
    }

    sample_df = pd.DataFrame(sample_data, index=[0])

    # Apply the same scaling as in training to continuous features
    if CONTINUOUS_FEATURES:
        sample_df[CONTINUOUS_FEATURES] = scaler.transform(sample_df[CONTINUOUS_FEATURES])

    # Ensure column order matches the training features
    if FEATURES:
        sample_df = sample_df[FEATURES]

    print("Regression single-sample input:")
    print(sample_df)

    # ------------------ 1.5 Load trained regression model ------------------ #
    with open(MODEL_PATH, 'rb') as f:
        model = pickle.load(f)

    # ------------------ 1.6 SHAP explainer and values ------------------ #

    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(sample_df)

    # For regression, shap_values is usually a 2D array: (n_samples, n_features)
    if isinstance(shap_values, list):
        shap_values = shap_values[0]

    shap_values_sample = shap_values[0]  # (n_features,)

    expected_value = explainer.expected_value
    if isinstance(expected_value, (np.ndarray, list)):
        base_value = expected_value[0]
    else:
        base_value = expected_value

    shap.initjs()

    exp = shap.Explanation(
        values=shap_values_sample,
        base_values=base_value,
        data=sample_df.iloc[0],
        feature_names=list(sample_df.columns)
    )

    # ------------------ 1.7 Plots for single-sample explanation ------------------ #

    # 1) Interactive force plot (HTML)
    force_plot = shap.force_plot(base_value, shap_values_sample, sample_df.iloc[0])
    shap.save_html(OUTPUT_FILES['force_html'], force_plot)
    print(f"Saved interactive force plot to: {OUTPUT_FILES['force_html']}")

    # 2) Static force plot (PNG)
    fig = plt.figure(figsize=(10, 3))
    shap.force_plot(
        base_value,
        shap_values_sample,
        sample_df.iloc[0],
        matplotlib=True,
        show=False
    )
    save_plot(fig, OUTPUT_FILES['force_png'])

    # 3) Waterfall plot (most important features for this sample)
    fig = plt.figure(figsize=(8, 6))
    shap.waterfall_plot(exp, max_display=10, show=False)
    save_plot(fig, OUTPUT_FILES['waterfall'])

    # 4) Bar plot (sorted SHAP values for this sample)
    fig = plt.figure(figsize=(8, 6))
    shap.bar_plot(exp.values, feature_names=exp.feature_names, max_display=10, show=False)
    save_plot(fig, OUTPUT_FILES['bar'])

    print("\nAll regression SHAP plots (single sample) have been generated.")


# ======================================================
# 2. Helper: build background from a single sample
#    (for classification or regression if needed)
# ======================================================

def build_background_from_single_sample(df_sample, n_background=50, noise_std=0.01):
    """
    Build a simple background dataset by replicating a single sample
    and adding small noise to numeric columns.

    This is a fallback option. In practice, it is better to use
    a subset of the real training data as background.
    """
    background = pd.concat([df_sample] * n_background, ignore_index=True)

    for col in background.columns:
        if np.issubdtype(background[col].dtype, np.number):
            background[col] = background[col] + np.random.normal(
                0, noise_std, size=background.shape[0]
            )

    return background


# ======================================================
# 3. XGBoost (or other tree-based) classification model SHAP analysis
#    Single-sample explanation
# ======================================================

def run_shap_for_xgboost_single_patient():
    """
    Run SHAP analysis for a single subject using a trained tree-based
    classification model (e.g., XGBClassifier).

    Assumptions:
    - Model is a tree-based classifier.
    - Feature names and encodings match those used in training.
    - By default, explain the positive class (index 1) in binary classification.
    """

    # ------------------ 3.1 Paths and I/O configuration ------------------ #
    # TODO: Replace these placeholders with your actual paths.
    MODEL_PATH = r"PATH_TO_YOUR_CLASSIFICATION_MODEL.joblib"
    RESULT_DIR = r"PATH_TO_OUTPUT_DIRECTORY_FOR_CLASSIFICATION_SHAP"
    os.makedirs(RESULT_DIR, exist_ok=True)

    # ------------------ 3.2 Single-sample input ------------------ #
    # TODO: Fill in sample_data with the same feature names and encodings
    #       as used during training.

    sample_data = {
        # "cat_feature_1": 1,
        # "cat_feature_2": 0,
        # "cont_feature_1": 50.0,
        # "cont_feature_2": 1.23,
        # ...
    }

    df_sample = pd.DataFrame([sample_data])
    print("Classification single-sample input:")
    print(df_sample)

    # ------------------ 3.3 Load trained classification model ------------------ #
    model = joblib.load(MODEL_PATH)

    # ------------------ 3.4 Build or load background data ------------------ #
    # Recommended: use a subset of your training data as background.
    # Here we show a simple fallback based on the single sample.
    background = build_background_from_single_sample(df_sample, n_background=50, noise_std=0.01)

    # ------------------ 3.5 SHAP explainer and values ------------------ #

    explainer = shap.TreeExplainer(model, data=background)
    shap_values = explainer.shap_values(df_sample)

    # For multi-class or list-style outputs, shap_values is a list of arrays.
    # For binary classification (newer SHAP versions), it can be an array.
    target_class_index = 1  # Adjust if you want to explain another class

    if isinstance(shap_values, list):
        # Multi-class or list output: pick one class
        shap_values_sample = shap_values[target_class_index][0]  # (n_features,)
        expected_value = explainer.expected_value[target_class_index]
    else:
        # Binary classification or single-output case
        shap_values_sample = shap_values[0]  # (n_features,)
        expected_value = explainer.expected_value

    if isinstance(expected_value, (np.ndarray, list)):
        base_value = expected_value[0]
    else:
        base_value = expected_value

    shap.initjs()

    exp = shap.Explanation(
        values=shap_values_sample,
        base_values=base_value,
        data=df_sample.iloc[0],
        feature_names=list(df_sample.columns)
    )

    # ------------------ 3.6 Plots for single-sample explanation ------------------ #

    force_html_path = os.path.join(RESULT_DIR, "shap_force_plot.html")
    force_png_path = os.path.join(RESULT_DIR, "shap_force_plot.png")
    waterfall_path = os.path.join(RESULT_DIR, "shap_waterfall_plot.png")
    bar_path = os.path.join(RESULT_DIR, "shap_bar_plot.png")

    # 1) Interactive force plot (HTML)
    force_plot = shap.force_plot(
        base_value,
        shap_values_sample,
        df_sample.iloc[0]
    )
    shap.save_html(force_html_path, force_plot)
    print(f"Saved interactive force plot to: {force_html_path}")

    # 2) Static force plot (PNG)
    fig = plt.figure(figsize=(10, 3))
    shap.force_plot(
        base_value,
        shap_values_sample,
        df_sample.iloc[0],
        matplotlib=True,
        show=False
    )
    save_plot(fig, force_png_path)

    # 3) Waterfall plot
    fig = plt.figure(figsize=(8, 6))
    shap.waterfall_plot(exp, max_display=10, show=False)
    save_plot(fig, waterfall_path)

    # 4) Bar plot
    fig = plt.figure(figsize=(8, 6))
    shap.bar_plot(
        exp.values,
        feature_names=exp.feature_names,
        max_display=10,
        show=False
    )
    save_plot(fig, bar_path)

    print("\nAll classification SHAP plots (single sample) have been generated.")


# ======================================================
# 4. Example main entry (optional)
# ======================================================

if __name__ == "__main__":
    # Uncomment the functions you want to run,
    # and make sure you have filled in paths and feature definitions above.

    # run_shap_for_lightgbm_single_patient()
    # run_shap_for_xgboost_single_patient()
    pass
